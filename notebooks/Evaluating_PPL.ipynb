{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import math\n",
    "from argparse import Namespace\n",
    "from typing import Sequence, Optional, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "\n",
    "from prekv import datautils, modelutils\n",
    "from prekv.quantizers import QuantizerBase, HiggsQuantizer\n",
    "from prekv.cache_utils import TreatPrefixSeparately,PredictorHiggsCache,SingleChunkQuantizedCacheWithPredictors\n",
    "from functools import partial\n",
    "from ppl import evaluate_perplexity\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arg_parser():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        default = \"unsloth/Llama-3.2-3B\",\n",
    "        type=str,\n",
    "        help=\"path to llama model to load, as in LlamaForCausalLM.from_pretrained()\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--torch_dtype\",\n",
    "        type=str,\n",
    "        default=\"auto\",\n",
    "        choices=[\"auto\", \"float16\", \"float32\", \"bfloat16\"],\n",
    "        help=\"dtype to load the model in\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_seqlen\",\n",
    "        type=int,\n",
    "        default=8192,\n",
    "        help=\"Model seqlen and calibration data context length.\",\n",
    "    )\n",
    "    parser.add_argument(\"--devices\",\n",
    "                        metavar=\"N\",\n",
    "                        type=str,\n",
    "                        nargs=\"+\",\n",
    "                        default=None,\n",
    "                        help=\"List of devices\")\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Seed for calibration data and initialization. \"\n",
    "             \"Note that the main training is not strictly deterministic.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ppl_chunk_size\", #<- need to be renamed\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ppl_buffer_size\",#<- need to be renamed\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--hadamard_groupsize\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Groupsize of Hadamard transform for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--edenn_d\",\n",
    "        type=int,\n",
    "        default=6,\n",
    "        help=\"The grid dimension d for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--edenn_n\",\n",
    "        type=int,\n",
    "        default=4096,\n",
    "        help=\"The grid size n for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\"--wandb\", action=\"store_true\", help=\"Whether to use wandb or store locally.\") #TODO: implement\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcec56e",
   "metadata": {},
   "source": [
    "### Parsing Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70031e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = make_arg_parser()\n",
    "torch.set_num_threads(min(16, torch.get_num_threads()))\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.devices is None:\n",
    "    if torch.cuda.is_available():\n",
    "        args.devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        args.devices = [torch.device(\"cpu\")]\n",
    "else:\n",
    "    args.devices = [torch.device(device_str) for device_str in args.devices]\n",
    "assert len(args.devices) == 1, \"parallelism is still WIP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3aced",
   "metadata": {},
   "source": [
    "## PPL evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb948d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_values = torch.load('../key_value_predictors.pt')\n",
    "key_predictors, value_predictors =  key_values[\"key_predictors\"], key_values[\"value_predictors\"]\n",
    "[key_predictors[i].to(args.devices[0]) for i in key_predictors]\n",
    "[value_predictors[i].to(args.devices[0]) for i in value_predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18483642",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    config = transformers.AutoConfig.from_pretrained(args.model_name)\n",
    "    model =  transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_name, config=config, torch_dtype=args.torch_dtype, low_cpu_mem_usage=True).to(args.devices[0])\n",
    "    tokenizer =  transformers.AutoTokenizer.from_pretrained(args.model_name, config=config, padding_side=\"left\")\n",
    "\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")['input_ids']\n",
    "    step_size = args.ppl_chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d837aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cache_factory = None\n",
    "    ppl = evaluate_perplexity(model, testenc, args.model_seqlen, device=args.devices[0], step_size=step_size, cache_factory=cache_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    quantizer = HiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "    cache_factory = lambda: TreatPrefixSeparately(prefix_size=4,\n",
    "                          prefix_cache=transformers.DynamicCache(),\n",
    "                          suffix_cache=PredictorHiggsCache(\n",
    "                          config=model.config, min_buffer_size=args.ppl_buffer_size, save_dequantized_values=True,\n",
    "                          make_quantized_cache=partial(\n",
    "                                SingleChunkQuantizedCacheWithPredictors, quantizer=quantizer,\n",
    "                                key_predictors=key_predictors, value_predictors=value_predictors\n",
    "                            )\n",
    "                        ))\n",
    "\n",
    "    ppl_quantized = evaluate_perplexity(model, testenc, args.model_seqlen, device=args.devices[0], step_size=step_size, cache_factory=cache_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PPL on with static cache {ppl}\\nPPL on with quantized cache {ppl_quantized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab67a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
