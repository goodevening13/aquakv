{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import math\n",
    "from argparse import Namespace\n",
    "from typing import Sequence, Optional, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "\n",
    "from prekv import datautils, modelutils\n",
    "from prekv.quantizers import QuantizerBase, HiggsQuantizer\n",
    "from prekv.linear_utils import fit_linear_regression\n",
    "from train_predictors import OutputCatcher, get_predictor, get_dequant_values, compute_relative_mse\n",
    "from ppl import evaluate_perplexity\n",
    "from datasets import load_dataset\n",
    "from prekv.cache_utils import TreatPrefixSeparately,PredictorHiggsCache,SingleChunkQuantizedCacheWithPredictors\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arg_parser():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        default = \"unsloth/Llama-3.2-3B\",\n",
    "        type=str,\n",
    "        help=\"path to llama model to load, as in LlamaForCausalLM.from_pretrained()\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        default=\"pajama\",\n",
    "        type=str,\n",
    "        help=\"Dataset name [c4, pajama] or path to data where to extract calibration data from.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--torch_dtype\",\n",
    "        type=str,\n",
    "        default=\"auto\",\n",
    "        choices=[\"auto\", \"float16\", \"float32\", \"bfloat16\"],\n",
    "        help=\"dtype to load the model in\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compute_dtype\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"dtype for computing activations\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_seqlen\",\n",
    "        type=int,\n",
    "        default=8192,\n",
    "        help=\"Model seqlen and calibration data context length.\",\n",
    "    )\n",
    "    parser.add_argument(\"--devices\",\n",
    "                        metavar=\"N\",\n",
    "                        type=str,\n",
    "                        nargs=\"+\",\n",
    "                        default=None,\n",
    "                        help=\"List of devices\")\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Seed for calibration data and initialization. \"\n",
    "             \"Note that the main training is not strictly deterministic.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--offload_activations\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Offload activations to RAM to save GPU memory.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--total_nsamples\",\n",
    "        type=int,\n",
    "        default=256,\n",
    "        help=\"Number of calibration data samples.If None take all calibration data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--valid_nsamples\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of calibration data samples.If None take all calibration data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chunk_size\",\n",
    "        type=int,\n",
    "        default=4096,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--percdamp\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help=\"Percent of the average Hessian diagonal to use for dampening.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--hadamard_groupsize\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Groupsize of Hadamard transform for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--edenn_d\",\n",
    "        type=int,\n",
    "        default=6,\n",
    "        help=\"The grid dimension d for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--edenn_n\",\n",
    "        type=int,\n",
    "        default=4096,\n",
    "        help=\"The grid size n for HIGGS.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--ppl_chunk_size\", #<- need to be renamed\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ppl_buffer_size\",#<- need to be renamed\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        type=str,\n",
    "        default=\"./key_value_predictors.pt\",\n",
    "        help=\"Path to save trained predictors for Key and Values\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--wandb\", action=\"store_true\", help=\"Whether to use wandb or store locally.\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcec56e",
   "metadata": {},
   "source": [
    "### Parsing Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70031e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = make_arg_parser()\n",
    "torch.set_num_threads(min(16, torch.get_num_threads()))\n",
    "args = parser.parse_args(args=[])\n",
    "args.offload_activations = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75be82",
   "metadata": {},
   "source": [
    "### Training from the scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer defaults\n",
    "if args.devices is None:\n",
    "    if torch.cuda.is_available():\n",
    "        args.devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        args.devices = [torch.device(\"cpu\")]\n",
    "else:\n",
    "    args.devices = [torch.device(device_str) for device_str in args.devices]\n",
    "assert len(args.devices) == 1, \"parallelism is still WIP\"\n",
    "\n",
    "# load model and data\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name, torch_dtype=args.torch_dtype, low_cpu_mem_usage=True, use_cache=False\n",
    ")\n",
    "\n",
    "data = datautils.get_loaders(\n",
    "    args.dataset,\n",
    "    nsamples=args.total_nsamples,\n",
    "    seed=args.seed,\n",
    "    model_path=args.model_name,\n",
    "    seqlen=args.model_seqlen,\n",
    ")\n",
    "\n",
    "quantizer = HiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "\n",
    "# Calibration: propagate a set of inputs through one layer at a time, train predictors as we go\n",
    "layers = modelutils.get_layers(model)\n",
    "\n",
    "inps, forward_args = modelutils.get_inps(\n",
    "    model, data, args.model_seqlen, args.devices, args.offload_activations)\n",
    "\n",
    "for k, v in forward_args.items():\n",
    "    forward_args[k] = v.to(args.devices[0]) if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "outs = [torch.zeros_like(inp_tensor, pin_memory=inp_tensor.is_pinned()) for inp_tensor in inps]\n",
    "old_attn_keys = None\n",
    "old_attn_values = None\n",
    "\n",
    "key_predictors = {}\n",
    "value_predictors = {}\n",
    "\n",
    "for layer_index in range(len(layers)):\n",
    "    print(f\"\\n---------------- Layer {layer_index} of {len(layers)} ----------------\")\n",
    "    layer_device_original = next(layers[layer_index].parameters()).device\n",
    "    layer_dtype_original = next(layers[layer_index].parameters()).dtype\n",
    "    layer = layers[layer_index].to(device=args.devices[0], dtype=args.compute_dtype or layer_dtype_original)\n",
    "\n",
    "    layer.self_attn.k_proj = OutputCatcher(layer.self_attn.k_proj, args.offload_activations)\n",
    "    layer.self_attn.v_proj = OutputCatcher(layer.self_attn.v_proj, args.offload_activations)\n",
    "\n",
    "    modelutils.update_outs_inplace_(args.devices, layer, inps, outs, **forward_args, compute_mse=False)\n",
    "\n",
    "    attn_keys = layer.self_attn.k_proj.outputs\n",
    "    assert all(elem.shape[0] == 1 for elem in attn_keys)\n",
    "    attn_keys = [elem[0] for elem in attn_keys]\n",
    "\n",
    "    attn_values = layer.self_attn.v_proj.outputs\n",
    "    assert all(elem.shape[0] == 1 for elem in attn_values)\n",
    "    attn_values = [elem[0] for elem in attn_values]\n",
    "\n",
    "    layer.self_attn.k_proj = layer.self_attn.k_proj.inner\n",
    "    layer.self_attn.v_proj = layer.self_attn.v_proj.inner\n",
    "\n",
    "    layers[layer_index] = layer.to(device=layer_device_original, dtype=layer_dtype_original)\n",
    "    del layer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    inps, outs = outs, inps\n",
    "\n",
    "    if layer_index == 0:\n",
    "        old_attn_keys = attn_keys\n",
    "        old_attn_values = attn_values\n",
    "        continue\n",
    "\n",
    "    ### training predictor below ###\n",
    "    key_predictor_inputs = list(old_attn_keys)\n",
    "    key_predictor, mse_train_keys, mse_valid_keys = get_predictor(args, key_predictor_inputs, attn_keys)\n",
    "    attn_keys = get_dequant_values(args, quantizer, key_predictor, key_predictor_inputs, attn_keys)\n",
    "    del key_predictor_inputs\n",
    "    key_predictors[layer_index] = key_predictor.cpu()\n",
    "    train_bits_keys = - math.log(mse_train_keys) / math.log(4)\n",
    "    valid_bits_keys = - math.log(mse_valid_keys) / math.log(4)\n",
    "    print(f'{layer_index=}\\tPREDICTOR_KEYS   \\t| relMSE train: {mse_train_keys:.4f} valid: {mse_valid_keys:.4f} '\n",
    "          f'| equiv.bits train: {train_bits_keys:.2f} valid: {valid_bits_keys:.2f}')\n",
    "    value_predictor_inputs = [\n",
    "        torch.cat([k_i, old_v_i], dim=-1) for k_i, old_v_i in zip(attn_keys, old_attn_values)]\n",
    "    value_predictor, mse_train_values, mse_valid_values = get_predictor(args, value_predictor_inputs, attn_values)\n",
    "    attn_values = get_dequant_values(args, quantizer, value_predictor, value_predictor_inputs, attn_values)\n",
    "    value_predictors[layer_index] = value_predictor.cpu()\n",
    "    del value_predictor_inputs\n",
    "    train_bits_values = - math.log(mse_train_values) / math.log(4)\n",
    "    valid_bits_values = - math.log(mse_valid_values) / math.log(4)\n",
    "    print(\n",
    "        f'{layer_index=}\\tPREDICTOR_VALUES \\t| relMSE train: {mse_train_values:.4f} valid: {mse_valid_values:.4f} '\n",
    "        f'| equiv.bits train: {train_bits_values:.2f} valid: {valid_bits_values:.2f}')\n",
    "\n",
    "    old_attn_keys, old_attn_values = attn_keys, attn_values\n",
    "\n",
    "torch.save(dict(key_predictors=key_predictors, value_predictors=value_predictors), args.output_path)\n",
    "print(\"Saved predictors to\", args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3aced",
   "metadata": {},
   "source": [
    "## PPL evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ef58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.to(args.devices[0])\n",
    "    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    config = transformers.AutoConfig.from_pretrained(args.model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name, config=config, padding_side=\"left\")\n",
    "    \n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")['input_ids']\n",
    "    step_size = args.ppl_chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d837aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cache_factory = None\n",
    "    ppl = evaluate_perplexity(model, testenc, args.model_seqlen, device=args.devices[0], step_size=step_size, cache_factory=cache_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    quantizer = HiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "    cache_factory = lambda: TreatPrefixSeparately(prefix_size=4,\n",
    "                          prefix_cache=transformers.DynamicCache(),\n",
    "                          suffix_cache=PredictorHiggsCache(\n",
    "                          config=model.config, min_buffer_size=args.ppl_buffer_size, save_dequantized_values=True,\n",
    "                          make_quantized_cache=partial(\n",
    "                                SingleChunkQuantizedCacheWithPredictors, quantizer=quantizer,\n",
    "                                key_predictors=key_predictors, value_predictors=value_predictors\n",
    "                            )\n",
    "                        ))\n",
    "\n",
    "    ppl_quantized = evaluate_perplexity(model, testenc, args.model_seqlen, device=args.devices[0], step_size=step_size, cache_factory=cache_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PPL on with static cache {ppl}\\nPPL on with quantized cache {ppl_quantized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7ae44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
