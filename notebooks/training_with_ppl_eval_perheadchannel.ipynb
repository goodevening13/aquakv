{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=6\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "\n",
    "%env HF_HOME=/mnt/LLM\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import math\n",
    "from argparse import Namespace\n",
    "from typing import Sequence, Optional, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "\n",
    "from prekv import datautils, modelutils\n",
    "from prekv.quantizers import QuantizerBase, HiggsQuantizer\n",
    "from prekv.linear_utils import fit_linear_regression\n",
    "from train_predictors import OutputCatcher, get_predictor, get_dequant_values, compute_relative_mse\n",
    "from ppl import evaluate_perplexity\n",
    "from datasets import load_dataset\n",
    "from prekv.cache_utils import TreatPrefixSeparately,PredictorHiggsCache,SingleChunkQuantizedCacheWithPredictors\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arg_parser():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        default = \"unsloth/Llama-3.2-3B\",\n",
    "        type=str,\n",
    "        help=\"path to llama model to load, as in LlamaForCausalLM.from_pretrained()\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        default=\"pajama\",\n",
    "        type=str,\n",
    "        help=\"Dataset name [c4, pajama] or path to data where to extract calibration data from.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--torch_dtype\",\n",
    "        type=str,\n",
    "        default=\"auto\",\n",
    "        choices=[\"auto\", \"float16\", \"float32\", \"bfloat16\"],\n",
    "        help=\"dtype to load the model in\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compute_dtype\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"dtype for computing activations\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_seqlen\",\n",
    "        type=int,\n",
    "        default=8192,\n",
    "        help=\"Model seqlen and calibration data context length.\",\n",
    "    )\n",
    "    parser.add_argument(\"--devices\",\n",
    "                        metavar=\"N\",\n",
    "                        type=str,\n",
    "                        nargs=\"+\",\n",
    "                        default=None,\n",
    "                        help=\"List of devices\")\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Seed for calibration data and initialization. \"\n",
    "             \"Note that the main training is not strictly deterministic.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--offload_activations\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Offload activations to RAM to save GPU memory.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--total_nsamples\",\n",
    "        type=int,\n",
    "        default=256,\n",
    "        help=\"Number of calibration data samples.If None take all calibration data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--valid_nsamples\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of calibration data samples.If None take all calibration data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chunk_size\",\n",
    "        type=int,\n",
    "        default=4096,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--percdamp\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "        help=\"Percent of the average Hessian diagonal to use for dampening.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--hadamard_groupsize\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Groupsize of Hadamard transform for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--edenn_d\",\n",
    "        type=int,\n",
    "        default=6,\n",
    "        help=\"The grid dimension d for HIGGS.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--edenn_n\",\n",
    "        type=int,\n",
    "        default=4096,\n",
    "        help=\"The grid size n for HIGGS.\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--ppl_chunk_size\", #<- need to be renamed\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ppl_buffer_size\",#<- need to be renamed\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of tokens in one chunk.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        type=str,\n",
    "        default=\"./key_value_predictors_perheadchannel.pt\",\n",
    "        help=\"Path to save trained predictors for Key and Values\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--wandb\", action=\"store_true\", help=\"Whether to use wandb or store locally.\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcec56e",
   "metadata": {},
   "source": [
    "### Parsing Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70031e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = make_arg_parser()\n",
    "torch.set_num_threads(min(16, torch.get_num_threads()))\n",
    "args = parser.parse_args(args=[])\n",
    "args.offload_activations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prekv.quantizers import QuantizerBase, HadLinear, quantize_linear_weight\n",
    "class PerChannelHiggsQuantizer(QuantizerBase):\n",
    "    def __init__(self, hadamard_groupsize: int, edenn_d: int, edenn_n: int):\n",
    "        super().__init__()\n",
    "        self.hadamard_groupsize, self.edenn_d, self.edenn_n = hadamard_groupsize, edenn_d, edenn_n\n",
    "        self.channel_group_size = 128\n",
    "        self.num_heads = model.config.num_key_value_heads\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def quantize(self, x: torch.Tensor):\n",
    "        x_flat = x.reshape(-1, x.shape[-1] // self.num_heads)  # [num tokens * num_heads, head_dim]\n",
    "        num_head_vectors, head_dim = x_flat.shape\n",
    "        total_block_size = self.channel_group_size * self.num_heads\n",
    "        \n",
    "        \n",
    "        padding_size = 0\n",
    "        if num_head_vectors % total_block_size != 0:\n",
    "            padding_size = total_block_size - num_head_vectors % total_block_size\n",
    "            x_flat = torch.cat([x_flat, torch.zeros(padding_size, head_dim, device=x.device, dtype=x.dtype)])\n",
    "\n",
    "        x_channelwise_with_padding = x_flat.reshape(\n",
    "            -1, total_block_size, head_dim).swapaxes(1, 2).flatten(0, 1).contiguous()\n",
    "        \n",
    "        quantized = quantize_linear_weight(\n",
    "            x_channelwise_with_padding, self.hadamard_groupsize, self.edenn_d, self.edenn_n)[0]\n",
    "        \n",
    "        quantized._head_dim = head_dim\n",
    "        quantized._size_before_pad = x_flat.shape[0] - padding_size\n",
    "        quantized._og_shape = x.shape\n",
    "        quantized._og_dtype = x.dtype\n",
    "        return quantized\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def dequantize(self, quantized: HadLinear) -> torch.Tensor:\n",
    "        total_block_size = self.channel_group_size * self.num_heads\n",
    "        x_channelwise_with_padding =  quantized(\n",
    "            torch.eye(quantized.weight.shape[1], device='cuda').half()).T[:, :total_block_size].contiguous()\n",
    "        \n",
    "        assert x_channelwise_with_padding.shape[-1] == total_block_size, \"dequantized shape mismatch; did you set hadamard_groupsize correctly?\"\n",
    "        x_flat = x_channelwise_with_padding.reshape(\n",
    "            -1, quantized._head_dim, total_block_size).swapaxes(1, 2).reshape(-1, quantized._head_dim)\n",
    "        x_flat = x_flat[:quantized._size_before_pad]\n",
    "        return x_flat.reshape(quantized._og_shape).to(quantized._og_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75be82",
   "metadata": {},
   "source": [
    "### Training from the scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer defaults\n",
    "if args.devices is None:\n",
    "    if torch.cuda.is_available():\n",
    "        args.devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        args.devices = [torch.device(\"cpu\")]\n",
    "else:\n",
    "    args.devices = [torch.device(device_str) for device_str in args.devices]\n",
    "assert len(args.devices) == 1, \"parallelism is still WIP\"\n",
    "\n",
    "# load model and data\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name, torch_dtype=args.torch_dtype, low_cpu_mem_usage=True, use_cache=False\n",
    ")\n",
    "\n",
    "data = datautils.get_loaders(\n",
    "    args.dataset,\n",
    "    nsamples=args.total_nsamples,\n",
    "    seed=args.seed,\n",
    "    model_path=args.model_name,\n",
    "    seqlen=args.model_seqlen,\n",
    ")\n",
    "\n",
    "key_quantizer = PerChannelHiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "value_quantizer = HiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "\n",
    "# Calibration: propagate a set of inputs through one layer at a time, train predictors as we go\n",
    "layers = modelutils.get_layers(model)\n",
    "\n",
    "inps, forward_args = modelutils.get_inps(\n",
    "    model, data, args.model_seqlen, args.devices, args.offload_activations)\n",
    "\n",
    "for k, v in forward_args.items():\n",
    "    forward_args[k] = v.to(args.devices[0]) if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "outs = [torch.zeros_like(inp_tensor, pin_memory=inp_tensor.is_pinned()) for inp_tensor in inps]\n",
    "old_attn_keys = None\n",
    "old_attn_values = None\n",
    "\n",
    "key_predictors = {}\n",
    "value_predictors = {}\n",
    "\n",
    "for layer_index in range(len(layers)):\n",
    "    print(f\"\\n---------------- Layer {layer_index} of {len(layers)} ----------------\")\n",
    "    layer_device_original = next(layers[layer_index].parameters()).device\n",
    "    layer_dtype_original = next(layers[layer_index].parameters()).dtype\n",
    "    layer = layers[layer_index].to(device=args.devices[0], dtype=args.compute_dtype or layer_dtype_original)\n",
    "\n",
    "    layer.self_attn.k_proj = OutputCatcher(layer.self_attn.k_proj, args.offload_activations)\n",
    "    layer.self_attn.v_proj = OutputCatcher(layer.self_attn.v_proj, args.offload_activations)\n",
    "\n",
    "    modelutils.update_outs_inplace_(args.devices, layer, inps, outs, **forward_args, compute_mse=False)\n",
    "\n",
    "    attn_keys = layer.self_attn.k_proj.outputs\n",
    "    assert all(elem.shape[0] == 1 for elem in attn_keys)\n",
    "    attn_keys = [elem[0] for elem in attn_keys]\n",
    "\n",
    "    attn_values = layer.self_attn.v_proj.outputs\n",
    "    assert all(elem.shape[0] == 1 for elem in attn_values)\n",
    "    attn_values = [elem[0] for elem in attn_values]\n",
    "\n",
    "    layer.self_attn.k_proj = layer.self_attn.k_proj.inner\n",
    "    layer.self_attn.v_proj = layer.self_attn.v_proj.inner\n",
    "\n",
    "    layers[layer_index] = layer.to(device=layer_device_original, dtype=layer_dtype_original)\n",
    "    del layer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    inps, outs = outs, inps\n",
    "\n",
    "    if layer_index == 0:\n",
    "        old_attn_keys = attn_keys\n",
    "        old_attn_values = attn_values\n",
    "        continue\n",
    "\n",
    "    ### training predictor below ###\n",
    "    key_predictor_inputs = list(old_attn_keys)\n",
    "    key_predictor, mse_train_keys, mse_valid_keys = get_predictor(args, key_predictor_inputs, attn_keys)\n",
    "    attn_keys = get_dequant_values(args, key_quantizer, key_predictor, key_predictor_inputs, attn_keys)\n",
    "    del key_predictor_inputs\n",
    "    key_predictors[layer_index] = key_predictor.cpu()\n",
    "    train_bits_keys = - math.log(mse_train_keys) / math.log(4)\n",
    "    valid_bits_keys = - math.log(mse_valid_keys) / math.log(4)\n",
    "    print(f'{layer_index=}\\tPREDICTOR_KEYS   \\t| relMSE train: {mse_train_keys:.4f} valid: {mse_valid_keys:.4f} '\n",
    "          f'| equiv.bits train: {train_bits_keys:.2f} valid: {valid_bits_keys:.2f}')\n",
    "    value_predictor_inputs = [\n",
    "        torch.cat([k_i, old_v_i], dim=-1) for k_i, old_v_i in zip(attn_keys, old_attn_values)]\n",
    "    value_predictor, mse_train_values, mse_valid_values = get_predictor(args, value_predictor_inputs, attn_values)\n",
    "    attn_values = get_dequant_values(args, value_quantizer, value_predictor, value_predictor_inputs, attn_values)\n",
    "    value_predictors[layer_index] = value_predictor.cpu()\n",
    "    del value_predictor_inputs\n",
    "    train_bits_values = - math.log(mse_train_values) / math.log(4)\n",
    "    valid_bits_values = - math.log(mse_valid_values) / math.log(4)\n",
    "    print(\n",
    "        f'{layer_index=}\\tPREDICTOR_VALUES \\t| relMSE train: {mse_train_values:.4f} valid: {mse_valid_values:.4f} '\n",
    "        f'| equiv.bits train: {train_bits_values:.2f} valid: {valid_bits_values:.2f}')\n",
    "\n",
    "    old_attn_keys, old_attn_values = attn_keys, attn_values\n",
    "\n",
    "torch.save(dict(key_predictors=key_predictors, value_predictors=value_predictors), args.output_path)\n",
    "print(\"Saved predictors to\", args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3aced",
   "metadata": {},
   "source": [
    "## PPL evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple, Optional, Dict, List\n",
    "from prekv.cache_utils import apply_rotary_to_keys,split_heads,combine_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e72ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleChunkQuantizedCacheWithPredictorsPerChannel(transformers.cache_utils.Cache):\n",
    "    \"\"\"A **write-once** cache that uses cumulative predictors; assumes that inputs are pre-grouped\"\"\"\n",
    "\n",
    "    def __init__(self, *, quantizer_key: QuantizerBase, quantizer_value: QuantizerBase,\n",
    "                 key_predictors: Optional[Dict[int, nn.Module]] = None,\n",
    "                 value_predictors: Optional[Dict[int, nn.Module]] = None):\n",
    "        super().__init__()\n",
    "        self.quantizer_key, self.quantizer_value, self.key_predictors, self.value_predictors = quantizer_key, quantizer_value, key_predictors, value_predictors\n",
    "        self.key_states_cache, self.value_states_cache = dict(), dict()\n",
    "        self.previous_key_reconstruction = self.previous_value_reconstruction = None\n",
    "        self.next_layer_idx = 0\n",
    "        self.seq_length = 0\n",
    "        self.cos = self.sin = None\n",
    "        self.head_dim = None\n",
    "\n",
    "    def predict_next_key_states(self) -> torch.Tensor:\n",
    "        if self.key_predictors is not None:\n",
    "            return self.key_predictors[self.next_layer_idx](self.previous_key_reconstruction)\n",
    "        else:\n",
    "            return torch.zeros_like(self.previous_key_reconstruction)\n",
    "\n",
    "    def predict_next_value_states(self, reconstructed_key_states: torch.Tensor) -> torch.Tensor:\n",
    "        if self.value_predictors is not None:\n",
    "            value_predictor_inputs = torch.cat([reconstructed_key_states, self.previous_value_reconstruction], dim=-1)\n",
    "            return self.value_predictors[self.next_layer_idx](value_predictor_inputs)\n",
    "        else:\n",
    "            return torch.zeros_like(self.previous_value_reconstruction)\n",
    "\n",
    "    def get_seq_length(self, layer_idx: int = 0) -> int:\n",
    "        assert layer_idx == 0\n",
    "        return self.key_states_cache[0].shape[-2] if self.key_states_cache else 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self,\n",
    "               key_states: Optional[torch.Tensor],\n",
    "               value_states: Optional[torch.Tensor],\n",
    "               layer_idx: int,\n",
    "               cache_kwargs: Optional[Dict[str, Any]] = None,\n",
    "               ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        assert layer_idx in (self.next_layer_idx, 0), (layer_idx, self.next_layer_idx, 0)\n",
    "        assert (key_states is None and value_states is None) or (key_states.shape == value_states.shape)\n",
    "        saving_new_entries = key_states is not None and key_states.numel() != 0\n",
    "        assert saving_new_entries == (layer_idx not in self.key_states_cache), \"can only write once per layer\"\n",
    "\n",
    "        if saving_new_entries:  # write mode\n",
    "            key_states_original, value_states_original = key_states, value_states\n",
    "            assert 'sin' in cache_kwargs and 'cos' in cache_kwargs\n",
    "            if self.cos is None:  # save the (identical) sin/cos for future reuse\n",
    "                self.cos, self.sin = cache_kwargs['cos'], cache_kwargs['sin']\n",
    "\n",
    "            if self.head_dim is None:\n",
    "                self.head_dim = key_states.shape[-1]\n",
    "            # undo rotation using cos(-alpha) = cos(alpha) and sin(-alpha) = -sin(alpha)\n",
    "            key_states = apply_rotary_to_keys(key_states, cos=self.cos, sin=-self.sin)\n",
    "\n",
    "            # v-- from [batch, num_heads, seq_length, head_dim] to [batch, seq_length, hidden_size]\n",
    "            key_states, value_states = map(combine_heads, (key_states, value_states))\n",
    "\n",
    "            if layer_idx == 0:\n",
    "                reconstructed_key_states = self.key_states_cache[0] = key_states\n",
    "                reconstructed_value_states = self.value_states_cache[0] = value_states\n",
    "            else:\n",
    "                predicted_key_states = self.predict_next_key_states()\n",
    "                self.key_states_cache[layer_idx] = self.quantizer_key.quantize(\n",
    "                    (key_states - predicted_key_states).flatten(0, -2))\n",
    "                reconstructed_key_states = predicted_key_states + self.quantizer_key.dequantize(\n",
    "                    self.key_states_cache[layer_idx]).view_as(key_states).to(predicted_key_states.dtype)\n",
    "                predicted_value_states = self.predict_next_value_states(reconstructed_key_states)\n",
    "                self.value_states_cache[layer_idx] = self.quantizer_value.quantize(\n",
    "                    (value_states - predicted_value_states).flatten(0, -2))\n",
    "                reconstructed_value_states = predicted_value_states + self.quantizer_value.dequantize(\n",
    "                    self.value_states_cache[layer_idx]).view_as(value_states).to(predicted_value_states.dtype)\n",
    "\n",
    "            # return original data since it's available, avoid quantization errors for that one step\n",
    "            result_key, result_value = key_states_original, value_states_original\n",
    "        else:  # read mode\n",
    "            if layer_idx == 0:\n",
    "                reconstructed_key_states = self.key_states_cache[0]\n",
    "                reconstructed_value_states = self.value_states_cache[0]\n",
    "            else:\n",
    "                reconstructed_key_states = self.predict_next_key_states()\n",
    "                reconstructed_key_states += self.quantizer_key.dequantize(\n",
    "                    self.key_states_cache[layer_idx]).view_as(\n",
    "                    reconstructed_key_states).to(reconstructed_key_states.dtype)\n",
    "\n",
    "                reconstructed_value_states = self.predict_next_value_states(reconstructed_key_states)\n",
    "                reconstructed_value_states += self.quantizer_value.dequantize(self.value_states_cache[layer_idx]).view_as(\n",
    "                    reconstructed_value_states).to(reconstructed_value_states.dtype)\n",
    "\n",
    "            # apply rotary embedding again\n",
    "            assert self.sin is not None and self.cos is not None and self.head_dim is not None\n",
    "            result_key_without_rotary = split_heads(reconstructed_key_states, self.head_dim)\n",
    "            result_key = apply_rotary_to_keys(result_key_without_rotary, cos=self.cos, sin=self.sin)\n",
    "            result_value = split_heads(reconstructed_value_states, self.head_dim)\n",
    "\n",
    "        self.next_layer_idx = layer_idx + 1\n",
    "        self.previous_key_reconstruction = reconstructed_key_states\n",
    "        self.previous_value_reconstruction = reconstructed_value_states\n",
    "        return result_key, result_value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.get_seq_length()})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ef58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.to(args.devices[0])\n",
    "    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    config = transformers.AutoConfig.from_pretrained(args.model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name, config=config, padding_side=\"left\")\n",
    "    \n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")['input_ids']\n",
    "    step_size = args.ppl_chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d837aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cache_factory = None\n",
    "    ppl = evaluate_perplexity(model, testenc, args.model_seqlen, device=args.devices[0], step_size=step_size, cache_factory=cache_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[key_predictors[i].to(args.devices[0]) for i in key_predictors]\n",
    "[value_predictors[i].to(args.devices[0]) for i in value_predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    key_quantizer = PerChannelHiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "    value_quantizer = HiggsQuantizer(args.hadamard_groupsize, args.edenn_d, args.edenn_n)\n",
    "    cache_factory = lambda: TreatPrefixSeparately(prefix_size=4,\n",
    "                          prefix_cache=transformers.DynamicCache(),\n",
    "                          suffix_cache=PredictorHiggsCache(\n",
    "                          config=model.config, min_buffer_size=args.ppl_buffer_size, save_dequantized_values=True,\n",
    "                          make_quantized_cache=partial(\n",
    "                                SingleChunkQuantizedCacheWithPredictorsPerChannel, \n",
    "                                quantizer_key=key_quantizer, quantizer_value=value_quantizer,\n",
    "                                key_predictors=key_predictors, value_predictors=value_predictors\n",
    "                            )\n",
    "                        ))\n",
    "\n",
    "    ppl_quantized = evaluate_perplexity(model, testenc, args.model_seqlen, device=args.devices[0], step_size=step_size, cache_factory=cache_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PPL on with static cache {ppl}\\nPPL on with quantized cache {ppl_quantized}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
